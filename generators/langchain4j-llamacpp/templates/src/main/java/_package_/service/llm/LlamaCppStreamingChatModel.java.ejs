package <%= packageName %>.service.llm;

import de.kherud.llama.InferenceParameters;
import de.kherud.llama.LlamaModel;
import de.kherud.llama.LlamaOutput;
import de.kherud.llama.ModelParameters;
import de.kherud.llama.args.MiroStat;
import dev.langchain4j.data.message.AiMessage;
import dev.langchain4j.data.message.ChatMessage;
import dev.langchain4j.model.StreamingResponseHandler;
import dev.langchain4j.model.chat.StreamingChatLanguageModel;
import dev.langchain4j.model.output.Response;

import java.nio.file.Path;
import java.util.List;

import static dev.langchain4j.internal.ValidationUtils.ensureNotBlank;
import static dev.langchain4j.internal.ValidationUtils.ensureNotEmpty;


public class LlamaCppStreamingChatModel implements StreamingChatLanguageModel {

    private final String modelHome;
    private final String modelName;

    public LlamaCppStreamingChatModel(String modelHome, String modelName) {
        this.modelHome = ensureNotBlank(modelHome, "modelHome");
        this.modelName = ensureNotBlank(modelName, "modelName");
        // log.info("LlamaCppStreamingChatModel created: model={}", Path.of(modelHome, modelName));
    }

    private ModelParameters createModelParameters() {
        return new ModelParameters()
            .setNGpuLayers(1)
            .setModelFilePath(Path.of(modelHome, modelName).toString());
    }

    private InferenceParameters createInferenceParameters(String prompt) {
        return new InferenceParameters(prompt)
            .setTemperature(0.7f)
            .setPenalizeNl(true)
            //                .setNProbs(10)
            .setMiroStat(MiroStat.V2);
    }

    @Override
    public void generate(List<ChatMessage> messages, StreamingResponseHandler<AiMessage> handler) {
        ensureNotEmpty(messages, "messages");

        String prompt = LlamaCppHelper.generatePrompt(messages);
        var modelParams = createModelParameters();
        var inferParams = createInferenceParameters(prompt);

        var contentBuilder = new StringBuilder();
        try (var model = new LlamaModel(modelParams)) {
            Iterable<LlamaOutput> outputs = model.generate(inferParams);
            for (LlamaOutput output : outputs) {
                handler.onNext(output.text);
                contentBuilder.append(output.text);
            }
            handler.onComplete(Response.from(AiMessage.from(contentBuilder.toString())));
        } catch (Exception e) {
            handler.onError(e);
        }
    }
}
