package <%= packageName %>.service.llm;

import de.kherud.llama.InferenceParameters;
import de.kherud.llama.LlamaModel;
import de.kherud.llama.LlamaOutput;
import de.kherud.llama.ModelParameters;
import de.kherud.llama.args.MiroStat;
import java.nio.file.Path;
import java.util.List;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import org.springframework.ai.chat.model.ChatModel;
import org.springframework.ai.chat.model.ChatResponse;
import org.springframework.ai.chat.model.Generation;
import org.springframework.ai.chat.model.StreamingChatModel;
import org.springframework.ai.chat.prompt.ChatOptions;
import org.springframework.ai.chat.prompt.Prompt;
import reactor.core.publisher.Flux;

public class LlamaCppChatClient implements ChatModel, StreamingChatModel {

    private final Logger logger = LoggerFactory.getLogger(getClass());

    private String modelHome;

    private String modelName;

    public String getModelHome() {
        return modelHome;
    }

    public void setModelHome(String modelHome) {
        this.modelHome = modelHome;
    }

    public String getModelName() {
        return modelName;
    }

    public void setModelName(String modelName) {
        this.modelName = modelName;
    }

    private ModelParameters createModelParameters() {
        return new ModelParameters()
            .setNGpuLayers(1)
            .setModelFilePath(Path.of(modelHome, modelName).toString());
    }

    private InferenceParameters createInferenceParameters(String prompt) {
        return new InferenceParameters(prompt)
            .setTemperature(0.7f)
            .setPenalizeNl(true)
            //                .setNProbs(10)
            .setMiroStat(MiroStat.V2);
    }

    @Override
    public ChatResponse call(Prompt prompt) {
        // LlamaModel.setLogger((level, message) -> System.out.print(message));
        var modelParams = createModelParameters();
        var inferParams = createInferenceParameters(prompt.getContents());
        var sb = new StringBuilder();
        try (var model = new LlamaModel(modelParams)) {
            Iterable<LlamaOutput> outputs = model.generate(inferParams);
            for (LlamaOutput output : outputs) {
                sb.append(output.text);
            }
        }
        return new ChatResponse(List.of(new Generation(sb.toString())));
    }

    @Override
    public ChatOptions getDefaultOptions() {
        return null;
    }

    @Override
    public Flux<ChatResponse> stream(Prompt prompt) {
        // logger.info("generateStream: prompt={}", prompt.getContents());
        // LlamaModel.setLogger((level, message) -> System.out.print(message));
        var modelParams = createModelParameters();
        var inferParams = createInferenceParameters(prompt.getContents());

        return Flux.using(
            () -> new LlamaModel(modelParams),
            model ->
                Flux
                    .fromIterable(model.generate(inferParams))
                    .map(output -> {
                    var text = output.text;
                    // System.out.print(text);
                    return new ChatResponse(List.of(new Generation(text)));
                }),
            LlamaModel::close
        );
    }
}
